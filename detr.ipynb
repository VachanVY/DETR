{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from typing import Sequence\n",
    "from scipy.optimize import linear_sum_assignment as scipy_lsa\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import jax\n",
    "from jax import (\n",
    "    Array,\n",
    "    numpy as jnp,\n",
    "    random as jrand,\n",
    "    lax\n",
    ")\n",
    "import keras as nn\n",
    "import keras_cv as ncv\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print('Python Version', sys.version); del sys\n",
    "print(f\"Keras Version {nn.__version__} with {nn.backend.backend()} backend \\tJax Version {jax.__version__}\")\n",
    "print(\"Jax backend device\", jax.default_backend())\n",
    "\n",
    "TEST_ALSO = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETR and Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids = [    \"no_object\",    \"Aeroplane\",    \"Bicycle\",    \"Bird\",    \"Boat\",    \"Bottle\",    \"Bus\",\n",
    "    \"Car\",    \"Cat\",    \"Chair\",    \"Cow\",    \"Dining Table\",    \"Dog\",    \"Horse\",    \"Motorbike\",\n",
    "    \"Person\",    \"Potted Plant\",    \"Sheep\",    \"Sofa\",    \"Train\",    \"Tv/monitor\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class config:\n",
    "    # object detection args\n",
    "    N:int = 42 ### max(number of objects you want the model to be able to predict)\n",
    "    image_size:tuple[int, int] = (500, 500)\n",
    "\n",
    "    # Loss Args\n",
    "    l1_weight:float = 5.\n",
    "    giou_weight:float = 2.\n",
    "\n",
    "    # Model hyperparameters\n",
    "    ## Backbone and Transformer hyperparameters\n",
    "    d_model:int = 64\n",
    "    num_layers:int = 5\n",
    "    num_heads:int = 8\n",
    "    dff_in:int = 4*d_model\n",
    "    dropout_rate:float = 0.0\n",
    "    maxlen:Sequence[int] = (\n",
    "        256, ### maximum(H1*W1) H1 and W1 image dimensions of output of conv_backbone\n",
    "        N\n",
    "    )\n",
    "    ## FeedForward hyperparameters\n",
    "    num_classes:int = len(class_ids)\n",
    "\n",
    "    # Training Arguments\n",
    "    ## Optimizer\n",
    "    num_steps:int = 7_000\n",
    "    weight_decay:float = 1e-4\n",
    "    init_lr:float = 1e-7\n",
    "    max_lr:float = 1e-3\n",
    "    min_lr:float = 0.1*max_lr\n",
    "    warmup_steps:int = num_steps//35\n",
    "\n",
    "    ## General Args\n",
    "    batch_size:int = 32\n",
    "    patience:int = 5\n",
    "    eval_steps:int = 100\n",
    "    eval_freq:int = 300\n",
    "\n",
    "config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
    "\n",
    "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
    "    inputs = next(iter(inputs.take(1)))\n",
    "    images, bounding_boxes = inputs[0], {\"classes\": inputs[1][0], \"boxes\": inputs[1][1]}\n",
    "    ncv.visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=value_range,\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        y_true=bounding_boxes,\n",
    "        scale=5,\n",
    "        font_scale=0.65,\n",
    "        line_thickness=1,\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        class_mapping=class_mapping,\n",
    "    )\n",
    "\n",
    "def unpackage_raw_tfds_inputs(inputs, bounding_box_format):\n",
    "    image = inputs[\"image\"]\n",
    "    boxes = ncv.bounding_box.convert_format(\n",
    "      inputs[\"objects\"][\"bbox\"],\n",
    "      images=image,\n",
    "      source=\"rel_yxyx\",\n",
    "      target=bounding_box_format,\n",
    "    )\n",
    "    bounding_boxes = {\n",
    "      \"classes\": tf.cast(inputs[\"objects\"][\"label\"] + 1, dtype=tf.float32), # + 1 to incorparate no_object class\n",
    "      \"boxes\": tf.cast(boxes, dtype=tf.float32),\n",
    "    }\n",
    "    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}\n",
    "\n",
    "def load_pascal_voc(split, dataset, bounding_box_format):\n",
    "    ds:tf.data.Dataset = tfds.load(dataset, split=split, with_info=False, shuffle_files=True)\n",
    "    ds = ds.map(lambda x: unpackage_raw_tfds_inputs(x, bounding_box_format=bounding_box_format), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "train_data = load_pascal_voc(split=\"train\", dataset=\"voc/2007\", bounding_box_format=\"xywh\")\n",
    "val_data = load_pascal_voc(split=\"validation\", dataset=\"voc/2007\", bounding_box_format=\"xywh\")\n",
    "test_data = load_pascal_voc(split=\"test\", dataset=\"voc/2007\", bounding_box_format=\"xywh\")\n",
    "\n",
    "train_data = train_data.concatenate(test_data); del test_data\n",
    "print(\"Number of Samples in\")\n",
    "print(\"\\tTraining Dataset :\", train_data.cardinality().numpy().tolist())\n",
    "print(\"\\tValidation Dataset :\", val_data.cardinality().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    X = ncv.layers.Resizing(*config.image_size, bounding_box_format=\"xywh\", pad_to_aspect_ratio=True)(x[\"images\"])\n",
    "    y = (\n",
    "        tf.pad([x[\"bounding_boxes\"][\"classes\"]], [[0,0], [config.N, config.N]])[0][config.N:-len(x[\"bounding_boxes\"][\"classes\"])],\n",
    "        tf.pad(x[\"bounding_boxes\"][\"boxes\"], [[config.N, config.N], [0,0]])[config.N:-len(x[\"bounding_boxes\"][\"boxes\"])]\n",
    "    )\n",
    "    return X, y\n",
    "\n",
    "train_ds = train_data.map(lambda x: preprocess(x), num_parallel_calls=tf.data.AUTOTUNE)#.shuffle(6000)\n",
    "val_ds = val_data.map(lambda x: preprocess(x), num_parallel_calls=tf.data.AUTOTUNE)#.shuffle(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(train_ds.ragged_batch(4, drop_remainder=True), bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(val_ds.ragged_batch(4, drop_remainder=True), bounding_box_format=\"xywh\", value_range=(0, 255), rows=2, cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bbox(x, y):\n",
    "    img = x\n",
    "    cls = y[0]\n",
    "    box = y[1]/max(config.image_size)\n",
    "    return img, (cls, box)\n",
    "\n",
    "def denormalize_bbox(x, y):\n",
    "    img = x\n",
    "    cls = y[0]\n",
    "    box = y[1]*max(config.image_size)\n",
    "    return img, (cls, box)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: normalize_bbox(x, y)).batch(config.batch_size, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(lambda x, y: normalize_bbox(x, y)).batch(config.batch_size, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE).repeat().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_iter:tf.data.Iterator = iter(train_ds)\n",
    "val_data_iter:tf.data.Iterator = iter(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Modules and get Module Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def module_summary(\n",
    "        module:nn.Model,\n",
    "        input_shapes:Sequence[tuple], \n",
    "        output_shapes:Sequence[tuple]|None=None,\n",
    "        only_build:bool=False,\n",
    "        return_module:bool=False,\n",
    "        print_model_output_shapes:bool=False\n",
    "    ):\n",
    "    randn_arr = lambda input_shapes: [jrand.uniform(jrand.PRNGKey(42), shape=shape) for shape in input_shapes]\n",
    "    inputs = [\n",
    "        randn_arr([shape])[0] if not isinstance(shape[0], (tuple, list)) else randn_arr(shape) for shape in input_shapes\n",
    "    ]\n",
    "    blablahh = module(*inputs, training=True)\n",
    "    if only_build:\n",
    "        return module\n",
    "    blablahh:Sequence[Array] = (blablahh,) if not isinstance(blablahh, (list, tuple)) else blablahh\n",
    "\n",
    "    print(\"-\"*100)\n",
    "    if print_model_output_shapes:\n",
    "        print(\"printing model output shapes\")\n",
    "        for o in blablahh:\n",
    "            print(o.shape)\n",
    "\n",
    "    assert all([true_shape==pred.shape for true_shape, pred in  zip(output_shapes, blablahh)]), \"Output shapes don't match. Investigate...\"\n",
    "    module.summary()\n",
    "    print(\"-\"*100)\n",
    "    return module if return_module else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETR CompOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding:\n",
    "    \"\"\"```\n",
    "    Sinusoidal Fixed Positional Embeddings\n",
    "    Args:\n",
    "        maxlen:int\n",
    "        dim:int\n",
    "    sinusoidal_embeddings: \n",
    "        pos_emb: shape(1, maxlen, dim)\n",
    "    get_freqs:\n",
    "        get_freqs: sin_freqs_shape(1, maxlen, 1, dim), cos_freqs_shape(1, maxlen, 1, dim)\n",
    "    ```\"\"\"\n",
    "    def __init__(self, maxlen:int, dim:int):\n",
    "        p, i = jnp.meshgrid(jnp.arange(float(maxlen)), jnp.arange(dim/2)*2)\n",
    "        theta = (p/1e4**(i/dim)).T\n",
    "\n",
    "        self.pos_emb = jnp.stack([jnp.sin(theta), jnp.cos(theta)], axis=-1)\n",
    "        self.pos_emb = self.pos_emb.reshape((maxlen, dim))[None] # (1, maxlen, dim)\n",
    "\n",
    "    def sinusoidal_embeddings(self):\n",
    "        return self.pos_emb # (1, maxlen, dim)\n",
    "    \n",
    "    def get_freqs(self):\n",
    "        sin_freqs = self.pos_emb[..., None, ::2].repeat(repeats=2, axis=-1)\n",
    "        cos_freqs = self.pos_emb[..., None, 1::2].repeat(repeats=2, axis=-1)\n",
    "        return sin_freqs, cos_freqs # (1, maxlen, 1, dim), (1, maxlen, 1, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETRTransformerAttSubBlock(nn.Model):\n",
    "    \"\"\"```\n",
    "    Inputs:\n",
    "        xq:Array   =shape=> shape(B, T|N, d_model)\n",
    "        xkv:Array  =shape=> shape(B, T|N, d_model)\n",
    "        qAdd:Array =shape=> shape(B, T|N, d_model)\n",
    "        kAdd:Array =shape=> shape(B, T|N, d_model)\n",
    "        training:bool\n",
    "    Outputs:\n",
    "        y:Array =shape=> shape(B, T|N, d_model)\n",
    "    ```\"\"\"\n",
    "    def __init__(self, config:config):\n",
    "        super().__init__()                              # MLA In-out shapes EXAMPLE:\n",
    "        self.att = nn.layers.MultiHeadAttention(        # q: (B, T, dim)\n",
    "            num_heads=config.num_heads,                 # k: (B, N, dim)  =Output=> o: (B, T, dim) \n",
    "            key_dim=config.d_model//config.num_heads,   # v: (B, N, dim)\n",
    "            dropout=config.dropout_rate\n",
    "        )\n",
    "        self.norm = nn.layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "    def call(self, xq:Array, xkv:Array, qAdd:Array, kAdd:Array, training:bool): # (B, T|N, d_model)\n",
    "        z = self.att(xq + qAdd, xkv + kAdd, xkv, training=training) # (B, T|N, d_model)\n",
    "        y = self.norm(xq) + z # (B, T|N, d_model)\n",
    "        return y # (B, T|N, d_model)\n",
    "\n",
    "########## Test ############\n",
    "if TEST_ALSO:\n",
    "    module_summary(\n",
    "        DETRTransformerAttSubBlock(config), \n",
    "        input_shapes=[(2, config.maxlen[0], config.d_model)]*4, \n",
    "        output_shapes=[(2, config.maxlen[0], config.d_model)]\n",
    "    )\n",
    "    module_summary(\n",
    "        DETRTransformerAttSubBlock(config), \n",
    "        input_shapes=[(2, config.maxlen[1], config.d_model)]*4, \n",
    "        output_shapes=[(2, config.maxlen[1], config.d_model)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETRTransformerFeedForwardSubBlock(nn.Model):\n",
    "    \"\"\"```\n",
    "    Inputs:\n",
    "        x:Array =shape=> shape(B, T or N, d_model)\n",
    "    Outputs:\n",
    "        y:Array =shape=> shape(B, T or N, d_model)\n",
    "    ```\"\"\"\n",
    "    def __init__(self, config:config):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential([\n",
    "            nn.layers.Dense(config.dff_in, use_bias=True),\n",
    "            nn.layers.Activation(nn.activations.relu),\n",
    "            nn.layers.Dense(config.d_model, use_bias=True),\n",
    "            nn.layers.Dropout(config.dropout_rate)\n",
    "        ])\n",
    "        self.norm = nn.layers.LayerNormalization(epsilon=1e-5)\n",
    "    \n",
    "    def call(self, x:Array, training:bool):\n",
    "        z = self.ffn(x, training=training) # (B, T, d_model)\n",
    "        y = self.norm(z) + x # (B, T, d_model)\n",
    "        return y # (B, T, d_model)\n",
    "\n",
    "########## Test ############\n",
    "if TEST_ALSO:\n",
    "    module_summary(\n",
    "        DETRTransformerFeedForwardSubBlock(config), \n",
    "        input_shapes=[(2, config.maxlen[0], config.d_model)]*1, \n",
    "        output_shapes=[(2, config.maxlen[0], config.d_model)]*1\n",
    "    )\n",
    "    module_summary(\n",
    "        DETRTransformerFeedForwardSubBlock(config), \n",
    "        input_shapes=[(2, config.maxlen[1], config.d_model)]*1, \n",
    "        output_shapes=[(2, config.maxlen[1], config.d_model)]*1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETRTransformerEncoderBlock(nn.Model):\n",
    "    \"\"\"```\n",
    "    Inputs:\n",
    "        xq:Array   => image_features    =shape=> shape(B, T, d_model)\n",
    "        xkv:Array  => image_features    =shape=> shape(B, T, d_model)\n",
    "        qAdd:Array => spatial_pos_embed =shape=> shape(B, T, d_model)\n",
    "        kAdd:Array => spatial_pos_embed =shape=> shape(B, T, d_model)\n",
    "        training:bool\n",
    "    Outputs:\n",
    "        y:Array =shape=> shape(B, T, d_model)\n",
    "    ```\"\"\"\n",
    "    def __init__(self, config:config):\n",
    "        super().__init__()\n",
    "        self.attsubblock = DETRTransformerAttSubBlock(config)\n",
    "        self.ffn = DETRTransformerFeedForwardSubBlock(config)\n",
    "                \n",
    "    def call(\n",
    "        self,\n",
    "        xq:Array,\n",
    "        xkv:Array,\n",
    "        qAdd:Array,\n",
    "        kAdd:Array,\n",
    "        training:bool\n",
    "    ):\n",
    "        z = self.attsubblock(xq, xkv, qAdd, kAdd, training=training)\n",
    "        y = self.ffn(z, training=training)\n",
    "        return y\n",
    "    \n",
    "########## Test ############\n",
    "if TEST_ALSO:\n",
    "    module_summary(\n",
    "        DETRTransformerEncoderBlock(config), \n",
    "        input_shapes=[(2, config.maxlen[0], config.d_model)]*4, \n",
    "        output_shapes=[(2, config.maxlen[0], config.d_model)]*1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETRTransformerDecoderBlock(nn.Model):\n",
    "    \"\"\"```\n",
    "    Inputs:\n",
    "        xq:Array         => decoder_output (object_queries initially) =shape=> shape(B, N, d_model)\n",
    "        xkv:list[Array]  => [xq, encoder_output]                      =shape=> [shape(B, N, d_model), shape(B, T, d_model)]\n",
    "        qAdd:list[Array] => [object_queries, object_queries]          =shape=> [shape(B, N, d_model), shape(B, N, d_model)]\n",
    "        kAdd:list[Array] => [object_queries, spatial_pos_embed]       =shape=> [shape(B, N, d_model), shape(B, T, d_model)]\n",
    "        training:bool\n",
    "    Outputs:\n",
    "        yq:Array => shape(B, N, d_model)\n",
    "    ```\"\"\"\n",
    "    def __init__(self, config:config):\n",
    "        super().__init__()\n",
    "        self.attsubblock1 = DETRTransformerAttSubBlock(config)\n",
    "        self.attsubblock2 = DETRTransformerAttSubBlock(config)\n",
    "        self.ffn = DETRTransformerFeedForwardSubBlock(config)\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        xq:Array, # shape(B, N, d_model)\n",
    "        xkv:Sequence[Array],  # [xq, encoder_output]\n",
    "        qAdd:Sequence[Array], # [object_queries, object_queries]\n",
    "        kAdd:Sequence[Array], # [object_queries, spatial_pos_embed]\n",
    "        training:bool\n",
    "    ):\n",
    "        zq = self.attsubblock1(xq, xkv[0], qAdd[0], kAdd[0], training=training)\n",
    "        zq = self.attsubblock2(zq, xkv[1], qAdd[1], kAdd[1], training=training)\n",
    "        yq = self.ffn(zq, training=training)\n",
    "        return yq\n",
    "    \n",
    "########## Test ############\n",
    "if TEST_ALSO:\n",
    "    module_summary(\n",
    "        DETRTransformerDecoderBlock(config),\n",
    "        input_shapes=\n",
    "            [(2, config.maxlen[1], config.d_model),\n",
    "            [(2, config.maxlen[1], config.d_model), (2, config.maxlen[0], config.d_model)],\n",
    "            [(2, config.maxlen[1], config.d_model), (2, config.maxlen[1], config.d_model)],\n",
    "            [(2, config.maxlen[1], config.d_model), (2, config.maxlen[0], config.d_model)]],\n",
    "        output_shapes=[(2, config.maxlen[1], config.d_model)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![qq](images/image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETRTransformer(nn.Model):\n",
    "    def __init__(self, config:config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.spatial_positional_encoding = PositionalEmbedding(\n",
    "            config.maxlen[0], config.d_model\n",
    "        ).sinusoidal_embeddings()[0] # (max_H*W=T, d_model)\n",
    "        self.encoder_blocks = [\n",
    "            DETRTransformerEncoderBlock(config) for _ in range(config.num_layers)\n",
    "        ]\n",
    "\n",
    "        self.object_queries = nn.layers.Embedding(config.maxlen[-1], config.d_model)(jnp.arange(config.maxlen[-1])) # (N, d_model)\n",
    "        self.decoder_blocks = [\n",
    "            DETRTransformerDecoderBlock(config) for _ in range(config.num_layers)\n",
    "        ]\n",
    "\n",
    "    def call(self, x:Array, training:bool): # (B, H*W=T, d_model)\n",
    "        B, T = x.shape[:-1]\n",
    "\n",
    "        spatial_positional_encoding = lax.broadcast(self.spatial_positional_encoding[:T], (B,)) # (B, T, d_model) <== (max_H*W, d_model)\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(\n",
    "                xq=x, xkv=x, qAdd=spatial_positional_encoding, kAdd=spatial_positional_encoding, training=training) # (B, T, d_model)\n",
    "        encoder_output = x # (B, T, d_model)\n",
    "\n",
    "        object_queries = lax.broadcast(self.object_queries, (B,)) # (B, N, d_model)\n",
    "        x = object_queries\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            x = decoder_block(\n",
    "                xq=x, xkv=[x, encoder_output], \n",
    "                qAdd=[object_queries, object_queries], kAdd=[object_queries, spatial_positional_encoding],\n",
    "                training=training\n",
    "            )\n",
    "        return x # (B, N, d_model)\n",
    "    \n",
    "########## Test ############\n",
    "if TEST_ALSO:\n",
    "    module_summary(\n",
    "        DETRTransformer(config),\n",
    "        input_shapes=[(2, config.maxlen[0], config.d_model)]*1,\n",
    "        output_shapes=[(2, config.maxlen[1], config.d_model)]*1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss CompOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoxLoss(bbox_true:Array, bbox_pred:Array, lambda_giou:float=1., lambda_l1:float=1.):\n",
    "    giou_loss = lambda_giou*ncv.losses.GIoULoss(bounding_box_format=\"xyWH\")(bbox_true, bbox_pred)\n",
    "    l1_loss = lambda_l1*nn.losses.MeanAbsoluteError()(bbox_true, bbox_pred)\n",
    "    return giou_loss + l1_loss\n",
    "\n",
    "def ClassLoss(class_true:Array, class_prob:Array, down_weight_no_object_class:bool=False):\n",
    "    sample_weight = jnp.where(class_true==0, 0.1, 1.) if down_weight_no_object_class else None # downsample no_object class by 10% if True\n",
    "    return nn.losses.SparseCategoricalCrossentropy()(class_true, class_prob, sample_weight=sample_weight) # -logprob(class_true * sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matcher:\n",
    "    def __init__(self, vmaped:bool=True):\n",
    "        self.vmaped:bool = vmaped\n",
    "\n",
    "    @staticmethod\n",
    "    @jax.jit\n",
    "    def MatchLoss(class_true:Array, class_prob:Array, bbox_true:Array, bbox_pred:Array):\n",
    "        \"\"\"```\n",
    "        Inouts:\n",
    "            class_true:Array => shape(,)\n",
    "            class_prob:Array => shape(,)\n",
    "            bbox_true:Array => shape(4,)\n",
    "            bbox_pred:Array => shape(4,)\n",
    "        Outputs:\n",
    "            match_loss:Array => shape(,)\n",
    "        ```\"\"\"\n",
    "        class_bool = (class_true!=0).astype(float) # int(not class_true==0)\n",
    "        # class_true = no_object = 0 => int(not True) = 0\n",
    "        # class_true != no_object != 0 => int(not False) = 1\n",
    "        \n",
    "        match_loss = -class_bool*class_prob + class_bool*BoxLoss(bbox_true[None], bbox_pred[None])\n",
    "        return match_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    @jax.jit\n",
    "    def compute_unbatched_cost_matrix(class_true:Array, class_prob:Array, bbox_true:Array, bbox_pred:Array):\n",
    "        \"\"\"```\n",
    "        Inputs:\n",
    "            class_true:Array => shape(N,)\n",
    "            class_prob:Array => shape(N, num_classes)\n",
    "            bbox_true:Array => shape(N, 4)\n",
    "            bbox_pred:Array => shape(N, 4)\n",
    "        Outputs:\n",
    "            unbatched_cost:Array => shape(N, N)\n",
    "        ```\"\"\"\n",
    "        N = class_true.shape[0]\n",
    "        cost_i = lambda i: jax.vmap(lambda j: Matcher.MatchLoss(\n",
    "            class_true[i],\n",
    "            class_prob[j, class_true[i].astype(int)],\n",
    "            bbox_true[i],\n",
    "            bbox_pred[j]\n",
    "        ), in_axes=0, out_axes=0)(jnp.arange(N))\n",
    "        unbatched_cost = jax.vmap(lambda i: cost_i(i), in_axes=0, out_axes=0)(jnp.arange(N))\n",
    "        return unbatched_cost # (N, N)\n",
    "    \n",
    "    @staticmethod\n",
    "    @jax.jit\n",
    "    def compute_batched_cost_matrix(class_true:Array, class_prob:Array, bbox_true:Array, bbox_pred:Array):\n",
    "        \"\"\"```\n",
    "        Inputs:\n",
    "            class_true:Array => shape(B, N)\n",
    "            class_prob:Array => shape(B, N, num_classes)\n",
    "            bbox_true:Array => shape(B, N, 4)\n",
    "            bbox_pred:Array => shape(B, N, 4)\n",
    "        Outputs:\n",
    "            C:Array => shape(B, N, N)\n",
    "        ```\"\"\"\n",
    "        batch_size = class_true.shape[0]\n",
    "        C = jax.vmap(lambda B: Matcher.compute_unbatched_cost_matrix(\n",
    "            class_true[B], class_prob[B], bbox_true[B], bbox_pred[B]\n",
    "        ), in_axes=0, out_axes=0)(jnp.arange(batch_size))\n",
    "        return C # (B, N, N)\n",
    "    \n",
    "    @staticmethod\n",
    "    @jax.jit\n",
    "    def unvmaped_compute_batched_cost_matrix(class_true:Array, class_prob:Array, bbox_true:Array, bbox_pred:Array):\n",
    "        \"\"\"```\n",
    "        Inputs:\n",
    "            class_true:Array => shape(B, N)\n",
    "            class_prob:Array => shape(B, N, num_classes)\n",
    "            bbox_true:Array => shape(B, N, 4)\n",
    "            bbox_pred:Array => shape(B, N, 4)\n",
    "        ```\"\"\"\n",
    "        N = class_true.shape[1]\n",
    "        batch_size = len(class_true)\n",
    "        C = jnp.zeros((batch_size, N, N))\n",
    "        for b in range(batch_size):\n",
    "            for i in range(N):\n",
    "                for j in range(N):\n",
    "                    C = C.at[b, i, j].set(\n",
    "                        Matcher.MatchLoss(\n",
    "                            class_true[b][i], \n",
    "                            class_prob[b][j, class_true[b][i].astype(int)], \n",
    "                            bbox_true[b][i],\n",
    "                            bbox_pred[b][j]\n",
    "                        )\n",
    "                    )\n",
    "        return C\n",
    "\n",
    "    @staticmethod\n",
    "    # Cannot jit this function as linear_sum_assignment is used which is a numpy function not a jax function\n",
    "    def match(class_true:Array, class_prob:Array, bbox_true:Array, bbox_pred:Array, vmaped:bool=True):\n",
    "        \"\"\"```\n",
    "        Inputs:\n",
    "            class_true:Array => shape(B, N)\n",
    "            class_prob:Array => shape(B, N, num_classes)\n",
    "            bbox_true:Array => shape(B, N, 4)\n",
    "            bbox_pred:Array => shape(B, N, 4)\n",
    "        Outputs:\n",
    "            matched_class_prob:Array => shape(B, N, num_classes)\n",
    "            matched_bbox_pred:Array => shape(B, N, 4)\n",
    "        ```\"\"\"\n",
    "        C:Array = Matcher.compute_batched_cost_matrix( # (B, N, N)\n",
    "                    class_true, class_prob, bbox_true, bbox_pred\n",
    "                ) if vmaped else Matcher.unvmaped_compute_batched_cost_matrix(\n",
    "                    class_true, class_prob, bbox_true, bbox_pred\n",
    "                )\n",
    "        to_indices = jnp.stack(list(map(lambda Cb: scipy_lsa(Cb)[1], C)))[..., None] # (B, N, 1)\n",
    "\n",
    "        matched_class_prob = jnp.take_along_axis(class_prob, to_indices, axis=1) # (B, N, num_classes)\n",
    "        matched_bbox_pred = jnp.take_along_axis(bbox_pred, to_indices, axis=1)   # (B, N, 4)\n",
    "        return matched_class_prob, matched_bbox_pred # (B, N, num_classes), (B, N, 4)\n",
    "    \n",
    "    def __call__(self, y_true:Sequence[Array], y_pred:Sequence[Array]):\n",
    "        (class_true, bbox_true), (class_prob, bbox_pred) = y_true, y_pred\n",
    "        (class_true, bbox_true) = lax.stop_gradient(class_true.astype(int)), lax.stop_gradient(bbox_true)\n",
    "        (class_prob, bbox_pred) = lax.stop_gradient(class_prob), lax.stop_gradient(bbox_pred)\n",
    "\n",
    "        y_matched_pred = Matcher.match(class_true, class_prob, bbox_true, bbox_pred, vmaped=self.vmaped)\n",
    "        return y_matched_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hungarian Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HungarianLoss(nn.Loss):\n",
    "    \"\"\"```\n",
    "        Inputs:\n",
    "            y_true:\n",
    "                class_true:Array => shape(B, N)\n",
    "                bbox_true:Array => shape(B, N, 4)\n",
    "            y_pred:\n",
    "                class_prob:Array => shape(B, N, num_classes)\n",
    "                bbox_pred:Array => shape(B, N, 4)\n",
    "        Outputs:\n",
    "            loss: Array\n",
    "    ```\"\"\"\n",
    "    def __init__(self, l1_weight:float, giou_weight:float):\n",
    "        super().__init__()\n",
    "        self.l1_weight = l1_weight\n",
    "        self.giou_weight = giou_weight\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def compute_loss(self, y_true:tuple[Array, Array], y_pred:tuple[Array, Array]):\n",
    "        class_true, bbox_true = y_true\n",
    "        class_prob, bbox_pred = y_pred\n",
    "\n",
    "        class_loss = ClassLoss(class_true, class_prob, down_weight_no_object_class=True) # (B, N), (B, N, num_classes)\n",
    "        box_loss = BoxLoss(bbox_true, bbox_pred, self.giou_weight, self.l1_weight)\n",
    "        loss = class_loss + box_loss\n",
    "        return loss\n",
    "    \n",
    "    def call(self, y_true:tuple[Array, Array], y_pred:tuple[Array, Array]):\n",
    "        return self.compute_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Matcher with Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test ############\n",
    "def test_matcher_and_lossfn(rand:bool=False, print_it:bool=False):\n",
    "    class_true, bbox_true = next(val_data_iter)[1]\n",
    "    class_true, bbox_true = jnp.array(class_true), jnp.array(bbox_true)\n",
    "    if rand:\n",
    "        class_prob = jrand.uniform(jrand.PRNGKey(42), shape=(*class_true.shape, config.num_classes))\n",
    "        bbox_pred = jrand.uniform(jrand.PRNGKey(42), shape=bbox_true.shape)\n",
    "        class_pred = class_prob.argmax(-1)\n",
    "    else:\n",
    "        class_pred = jrand.permutation(jrand.PRNGKey(42), class_true, axis=1); atol=1e-3\n",
    "        bbox_pred = (jrand.permutation(jrand.PRNGKey(42), bbox_true, axis=1)+atol).clip(min=0, max=1)\n",
    "        class_prob = jax.nn.softmax(jax.nn.one_hot(class_pred, config.num_classes)+2)\n",
    "        assert (class_prob.argmax(-1)==class_pred).all()\n",
    "    \n",
    "    t0 = time.time()\n",
    "    mcls, mbbox = Matcher(vmaped=True)(y_true=(class_true, bbox_true), y_pred=(class_prob, bbox_pred))\n",
    "    t1 = time.time(); dt = t1-t0\n",
    "    print(f\"\\tTiming with batch_size {config.batch_size}: {(dt)*1000:.2f}ms or {(dt):.4f}s\")\n",
    "    print(f\"\\tTiming per batch_size: {(dt/config.batch_size)*1000:.2f}ms\")\n",
    "\n",
    "    def print_test_cases():\n",
    "        print(\"class_pred\", class_pred, sep=\"\\n\") # (B, 42)\n",
    "        print(\"bbox_pred\", bbox_pred, sep=\"\\n\")   # (B, 42, 4)\n",
    "        print(\"class_true\", class_true, sep=\"\\n\") # (B, 42)\n",
    "        print(\"matched_class_pred\", mcls.argmax(-1), sep=\"\\n\")\n",
    "        print(\"matched_bbox_pred\", mbbox, sep=\"\\n\")\n",
    "        print(\"bbox_true\", bbox_true, sep=\"\\n\")   # (B, 42, 4)\n",
    "    if print_it:\n",
    "        print_test_cases()\n",
    "    if not rand:\n",
    "        try:\n",
    "            max_diff = round((mbbox-bbox_true).max(), 4)\n",
    "            assert (mcls.argmax(-1)==class_true).all(), \"Problem in Matching Classes\"\n",
    "            assert max_diff<=(atol), f\"Problem in Matching Bounding Boxes max_diff: {max_diff} atol: {atol}\"\n",
    "        except:\n",
    "            print(\"Problem in matching. Investigate...\")\n",
    "            print_test_cases()\n",
    "            assert False\n",
    "\n",
    "    print(\n",
    "        \"\\tLoss without matching:\",\n",
    "        HungarianLoss(config.l1_weight, config.giou_weight)(\n",
    "            y_true=(class_true, bbox_true), y_pred=(class_prob, bbox_pred)\n",
    "        ), end=\"\\n\\t\"\n",
    "    )\n",
    "    print(\n",
    "        \"Loss with matching:\",\n",
    "        HungarianLoss(config.l1_weight, config.giou_weight)(\n",
    "            y_true=(class_true, bbox_true), y_pred=(mcls, mbbox)\n",
    "        )\n",
    "    )\n",
    "    return\n",
    "\n",
    "\n",
    "if TEST_ALSO:\n",
    "    print(\"First call:\")\n",
    "    test_matcher_and_lossfn(rand=False)\n",
    "    print(\"\\n\\nSubsequent call:\")\n",
    "    test_matcher_and_lossfn(rand=False)\n",
    "    print(\"\\n\\nCall on random predictions:\")\n",
    "    test_matcher_and_lossfn(rand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test ############ ==> (custom inputs)\n",
    "def test_matcher(print_it:bool=False):\n",
    "    # possible classes = [0, 1, 2]\n",
    "    class_true = jnp.array([[1, 2, 0], \n",
    "                            [2, 1, 0]])\n",
    "    print(f\"\\tclass_true, {class_true.shape}\") if print_it else print(\"\", end=\"\") # (B, N)\n",
    "    class_pred = jnp.array([[1, 0, 2], \n",
    "                            [0, 1, 2]])\n",
    "    print(f\"\\tclass_pred {class_pred.shape}\") if print_it else print(\"\", end=\"\") # (B, N)\n",
    "    N = class_true.shape[-1]\n",
    "\n",
    "    class_prob = jnp.array([[[0.1, 0.8, 0.1], # (B, N, n_classes)\n",
    "                            [0.8, 0.0, 0.2],\n",
    "                            [0.1, 0.0, 0.9]],\n",
    "                        [[0.9, 0.0, 0.1],\n",
    "                            [0.4, 0.5, 0.1],\n",
    "                            [0.3, 0.1, 0.6]]], dtype=jnp.float32)\n",
    "    print(f\"\\tclass_prob {class_prob.shape}\") if print_it else print(\"\", end=\"\")\n",
    "    bbox_true = jnp.array([[[0.2, 0.1, 0.6, 0.9],           # 1\n",
    "                        [0.1, 0.4, 0.5, 0.6],            # 2\n",
    "                        [0.0, 0.0, 0.0, 0.0]],           # 0\n",
    "                        [[0.1, 0.6, 0.5, 0.2],        # 2\n",
    "                            [0.1, 0.3, 0.5, 0.4],        # 1\n",
    "                            [0.0, 0.0, 0.0, 0.0]]],      # 0\n",
    "                                dtype=jnp.float32) # (B, N, 4)\n",
    "    print(f\"\\tbbox_true {bbox_true.shape}\") if print_it else print(\"\", end=\"\")\n",
    "    bbox_pred = jnp.array([[[0.198, 0.1, 0.601, 0.91],                           # 1\n",
    "                        [0.01, 0.009, 0.001, 0.0],                            # 0\n",
    "                        [0.101, 0.39, 0.501, 0.601]],                         # 2\n",
    "                        [[0.01, 0.009, 0.001, 0.0],                    # 0\n",
    "                        [0.11, 0.298, 0.499, 0.39],                   # 1\n",
    "                        [0.11, 0.62, 0.501, 0.2009]]],                # 2\n",
    "                                dtype=jnp.float32) # (B, N, 4)\n",
    "    print(f\"\\tbbox_pred {bbox_pred.shape}\") if print_it else print(\"\", end=\"\")\n",
    "\n",
    "    print(\"\\n\\tmaximum number of onbjects that can be detected is\", N) if print_it else print(\"\", end=\"\")\n",
    "    print(\"\\tunique classes\", jnp.unique(class_true), end=\"\\n\\n\") if print_it else print(\"\", end=\"\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    matched_class_prob1, matched_bbox_pred1 = Matcher(vmaped=False)(y_true=(class_true, bbox_true), y_pred=(class_prob, bbox_pred))\n",
    "    t1 = time.time()\n",
    "    matched_class_prob, matched_bbox_pred = Matcher(vmaped=True)(y_true=(class_true, bbox_true), y_pred=(class_prob, bbox_pred))\n",
    "    t2 = time.time()\n",
    "    assert all(\n",
    "        [jnp.allclose(matched_class_prob1,matched_class_prob), jnp.allclose(matched_bbox_pred1, matched_bbox_pred)]\n",
    "    ), \"Vmaped and unvmaped versions don't match. Investigate...\"\n",
    "    print(f\"\\tUnvmaped Timing: {(t1-t0)*1000:.2f}ms\")\n",
    "    print(f\"\\tVmaped   Timing: {(t2-t1)*1000:.2f}ms\\n\")\n",
    "\n",
    "    print(\n",
    "        \"\\tLoss without matching:\",\n",
    "        HungarianLoss(config.l1_weight, config.giou_weight)(\n",
    "            y_true=(class_true, bbox_true), y_pred=(class_prob, bbox_pred)\n",
    "        ), end=\"\\n\"\n",
    "    )\n",
    "    print(\n",
    "        \"\\tLoss with matching:\",\n",
    "        HungarianLoss(config.l1_weight, config.giou_weight)(\n",
    "            y_true=(class_true, bbox_true), y_pred=(matched_class_prob, matched_bbox_pred)\n",
    "        )\n",
    "    )\n",
    "    return\n",
    "\n",
    "if TEST_ALSO:\n",
    "    print(\"First Call:\")\n",
    "    test_matcher()\n",
    "    print(\"\\nSubsequent Call:\")\n",
    "    test_matcher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETR(nn.Model):\n",
    "    def __init__(self, config:config):\n",
    "        super().__init__()\n",
    "        # Convolutional backbone\n",
    "        ## shut warnings due to different input shape other than (224, 224) in conv backbone\n",
    "        import warnings; warnings.filterwarnings(\"ignore\")\n",
    "        self.conv_backbone = nn.Sequential([\n",
    "            nn.applications.MobileNetV3Small( # MobileNetV3Small, MobileNetV3Large\n",
    "                input_shape=(*config.image_size, 3),\n",
    "                include_top=False,\n",
    "                dropout_rate=0,\n",
    "                include_preprocessing=True # includes rescaling\n",
    "            ),\n",
    "            nn.layers.Conv2D(config.d_model, kernel_size=(1, 1)),\n",
    "            nn.layers.Reshape(target_shape=(-1, config.d_model))\n",
    "        ], name=\"convolutional_backbone\"); del warnings\n",
    "        # Transformer\n",
    "        self.transformer = DETRTransformer(config, name=\"detr_transformer\")\n",
    "        # FeedForward\n",
    "        self.linear_class, self.linear_bbox = (\n",
    "            nn.layers.Dense(config.num_classes, name=\"linear_class\"), \n",
    "            nn.layers.Dense(4, name=\"linear_bbox\")\n",
    "        )\n",
    "\n",
    "    def call(self, x:Array, training:bool=True): # (B, H, W, 3) or (B, 500, 500, 3)\n",
    "        z = self.conv_backbone(x, training=training) # (B, T=H1*W1, d_model)\n",
    "        z = self.transformer(z, training=training) # (B, N, d_model)\n",
    "        class_probs, bbox_pred = (\n",
    "            jax.nn.softmax(self.linear_class(z), axis=-1),\n",
    "            jax.nn.sigmoid(self.linear_bbox(z))\n",
    "        )\n",
    "        return class_probs, bbox_pred # (B, N, num_classes), (B, N, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = module_summary(\n",
    "    DETR(config),\n",
    "    input_shapes=[(2, *config.image_size, 3)],\n",
    "    output_shapes=[(2, config.maxlen[1], config.num_classes), (2, config.maxlen[1], 4)],\n",
    "    return_module=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essentials: Loss Function, Optimizer, Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = nn.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=config.min_lr,\n",
    "    decay_steps=config.num_steps,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    warmup_target=config.max_lr,\n",
    "    alpha=config.min_lr/config.max_lr\n",
    ")\n",
    "steps = jnp.arange(config.num_steps)\n",
    "lrs = jax.vmap(learning_rate)(steps)\n",
    "\n",
    "plt.plot(steps, lrs); del lrs, steps\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Steps\"); plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Learning Rate Vs Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nn.optimizers.AdamW(\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "loss_fn = HungarianLoss(\n",
    "    l1_weight=config.l1_weight,\n",
    "    giou_weight=config.giou_weight\n",
    ")\n",
    "match_true_predicted = Matcher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit           # (B, N)         (B, N, num_classes)\n",
    "def get_accuracy(class_true:Array, matched_class_prob:Array):\n",
    "    matched_class_pred = matched_class_prob.argmax(-1) # (B, N)\n",
    "    batch_accuracy = (class_true==matched_class_pred).sum(-1)/class_true.shape[-1] # (B,)\n",
    "    return batch_accuracy.mean() # (,)\n",
    "\n",
    "@jax.jit\n",
    "def model_call(trainable_vars:list, non_trainable_vars:list, X_batch:list):\n",
    "    y_pred, non_trainable_vars = model.stateless_call(trainable_vars, non_trainable_vars, X_batch)\n",
    "    return y_pred, non_trainable_vars\n",
    "\n",
    "# @jax.jit\n",
    "def optimizer_apply(optimizer_vars:list, grads:Sequence, trainable_vars:list):\n",
    "    print(16)\n",
    "    trainable_vars, optimizer_vars = optimizer.stateless_apply(\n",
    "        optimizer_vars, grads, trainable_vars\n",
    "    )\n",
    "    print(17)\n",
    "    return trainable_vars, optimizer_vars\n",
    "\n",
    "# has a numpy function in it (in the matcher), can't jit this function. So we jit individual comps\n",
    "def compute_loss_accuracy(trainable_vars:list, non_trainable_vars:list, X_batch:Array, y_batch:Sequence[Array]):\n",
    "    print(11)\n",
    "    y_pred, non_trainable_vars = model_call(trainable_vars, non_trainable_vars, X_batch)\n",
    "    print(12)\n",
    "    y_pred_matched = match_true_predicted(y_batch, y_pred) # has lax.stop_gradient, so no problem to grad the function `compute_loss_and_updates`\n",
    "    print(13)\n",
    "    class_accuracy = get_accuracy(y_batch[0], y_pred_matched[0])\n",
    "    print(14)\n",
    "    loss = loss_fn(y_batch, y_pred_matched)\n",
    "    print(loss, class_accuracy, non_trainable_vars)\n",
    "    return loss, class_accuracy, non_trainable_vars\n",
    "\n",
    "# has a numpy function in it (in the matcher), can't jit this function\n",
    "grad_fn = jax.value_and_grad(compute_loss_accuracy, has_aux=True, argnums=0) # returns ∂(loss)/∂(trainable_vars)\n",
    "\n",
    "def train_step(states:list, X_batch:Array, y_batch:Sequence[Array]):\n",
    "    print(1)\n",
    "    trainable_vars, non_trainable_vars, optimizer_vars = states\n",
    "    print(2)\n",
    "    (loss, class_accuracy, non_trainable_vars), grads = grad_fn(trainable_vars, non_trainable_vars, X_batch, y_batch)\n",
    "    print(3)\n",
    "    # trainable_vars, optimizer_vars = optimizer_apply(optimizer_vars, grads, trainable_vars)\n",
    "    trainable_vars, optimizer_vars = optimizer.stateless_apply(\n",
    "        optimizer_vars, grads, trainable_vars\n",
    "    )\n",
    "    print(4)\n",
    "    states = (trainable_vars, non_trainable_vars, optimizer_vars)\n",
    "    return loss, class_accuracy, states\n",
    "\n",
    "def evaluate(states:list):\n",
    "    mean_losses = []; mean_class_accuracies = []\n",
    "    for eval_batch_iter in [train_data_iter, val_data_iter]:\n",
    "        losses = jnp.zeros(config.eval_steps)\n",
    "        accuracies = jnp.zeros(config.eval_steps)\n",
    "\n",
    "        for eval_step in range(config.eval_steps):\n",
    "            X_batch, y_batch = next(eval_batch_iter)\n",
    "            loss, class_accuracy, non_trainable_vars = compute_loss_accuracy(*states[:-1], jnp.array(X_batch), jnp.array(y_batch))\n",
    "            states[1] = non_trainable_vars\n",
    "            losses = losses.at[eval_step].set(loss)\n",
    "            accuracies = accuracies.at[eval_step].set(class_accuracy)\n",
    "        mean_losses.append(losses.mean()); mean_class_accuracies.append(accuracies.mean())\n",
    "    return mean_losses, mean_class_accuracies # ([train_loss, val_loss], [train_accuracy, val_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(return_best_states:bool):\n",
    "    optimizer.build(model.trainable_variables)\n",
    "    states = [model.trainable_variables, model.non_trainable_variables, optimizer.variables]\n",
    "    training_losses = {\"train\":[]}; best_val_loss = 1e8; best_states = states\n",
    "\n",
    "    t0 = time.time(); step = 1; wait = 0\n",
    "    while True:\n",
    "        # condition to terminate\n",
    "        if step>config.num_steps or wait>config.patience:\n",
    "            print(f\"Early Stopping at Step {step}.\" if wait > config.patience else \"Training Terminated.\")\n",
    "            break\n",
    "        \n",
    "        # train model\n",
    "        X_batch, y_batch = next(train_data_iter); print(\"Got the data...\")\n",
    "        loss, cls_acc, states = train_step(states, jnp.array(X_batch), [jnp.array(yi) for yi in y_batch])\n",
    "        print(\"trained a step...\")\n",
    "        \n",
    "        training_losses[\"train\"].append(loss.tolist())\n",
    "        if step % config.eval_freq == 0:\n",
    "            print(\"Evaluating...\")\n",
    "            [eval_train_loss, eval_val_loss], [eval_train_acc, eval_val_acc] = evaluate(states)\n",
    "            print(f\"\\t| Training Loss:           {eval_train_loss:.4f}|| Validation Loss: {eval_val_loss:.4f}          |\")\n",
    "            print(f\"\\t| Training Class Accuracy: {eval_train_acc:.4f} || Validation Class Accuracy: {eval_val_acc:.4f} |\")\n",
    "\n",
    "            if eval_train_loss < best_val_loss:\n",
    "                    best_val_loss = eval_train_loss\n",
    "                    best_states = states\n",
    "                    best_step = step\n",
    "                    wait = 0\n",
    "            else:\n",
    "                wait += 1\n",
    "        \n",
    "        # calculate time\n",
    "        t1 = time.time()\n",
    "        dt = t1-t0; t0=t1\n",
    "\n",
    "        # print the essentials\n",
    "        print(f\"| Step: {step} || Loss: {loss:.4f} || Class Accuracy: {cls_acc:.4f} || LR: {learning_rate(step):e}||dt: {dt*1000:.2f}ms or {dt:.2f}s |\")\n",
    "        step += 1\n",
    "\n",
    "        if return_best_states:\n",
    "            print(f\"Best Weights are from Step {best_step} with best estimated validation loss of {best_val_loss}\")\n",
    "            states = best_states\n",
    "        \n",
    "    return states[:-1], training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, losses = train(return_best_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Vs LR and Test Training\n",
    "\n",
    "```python\n",
    "def LossVsLr_TestTraining(log10lr_range:tuple[float, float]):\n",
    "    model = module_summary(DETR(config), input_shapes=[(2, 500, 500, 3)], only_build=True)\n",
    "    model_call = jax.jit(model.stateless_call)\n",
    "    optimizer_lr = lambda lr: nn.optimizers.AdamW(\n",
    "        learning_rate=lr,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    loss_fn = HungarianLoss(\n",
    "        l1_weight=config.l1_weight, \n",
    "        giou_weight=config.giou_weight\n",
    "    )\n",
    "    match_true_predicted = Matcher()\n",
    "    \n",
    "    def trainStep(lr:float, state:Sequence[list], X_batch:Array, y_batch:Sequence[Array]):\n",
    "        [trainable_vars, non_trainable_vars] = state\n",
    "        optimizer = optimizer_lr(lr)\n",
    "        optimizer.build(model.trainable_variables)\n",
    "        optimizer_vars = optimizer.variables\n",
    "\n",
    "        def compute_loss(trainable_vars:list, non_trainable_vars:list, X_batch:Array, y_batch:Sequence[Array]):\n",
    "            y_pred, non_trainable_vars = model_call(trainable_vars, non_trainable_vars, X_batch)\n",
    "            y_pred_matched = match_true_predicted(y_batch, y_pred)\n",
    "            loss = loss_fn(y_batch, y_pred_matched)\n",
    "            return loss, non_trainable_vars\n",
    "        grad_fn = jax.value_and_grad(compute_loss, has_aux=True, argnums=0)\n",
    "        \n",
    "        (loss, non_trainable_vars), grads = grad_fn(trainable_vars, non_trainable_vars, X_batch, y_batch)\n",
    "        trainable_vars, optimizer_vars = optimizer.stateless_apply(optimizer_vars, grads, trainable_vars)\n",
    "        state = [trainable_vars, non_trainable_vars]\n",
    "        return loss, state\n",
    "    \n",
    "    lrs = (10**jnp.linspace(log10lr_range[0], log10lr_range[1], 100)).tolist()\n",
    "    losses = []\n",
    "    state = (model.trainable_variables, model.non_trainable_variables)\n",
    "    x, y = next(testing_iter); x, y = jnp.array(x), [jnp.array(yi) for yi in y]\n",
    "    t0 = time.time()\n",
    "    for step, lr in enumerate(lrs):\n",
    "        loss, state = trainStep(lr, state, x, y)\n",
    "        t1 = time.time(); losses.append(loss)\n",
    "        print(f\"| Step: {step+1} | Loss: {loss:.4f} | LR: {lr:e} | Time: {(t1-t0)*1000:.2f}ms |\")\n",
    "        t0=t1\n",
    "    return lrs, losses\n",
    "\n",
    "lrs, losses = LossVsLr_TestTraining((-6, -2))\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.xlabel(\"Log base10 Learning Rate: Do 10^(x) to get actual learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xticks([-6, -5, -4]+jnp.arange(-3.5, -2.0, 0.1).tolist())\n",
    "plt.plot(jnp.log10(jnp.array(lrs)).tolist(), losses)\n",
    "plt.show()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
